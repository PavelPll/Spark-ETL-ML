In order to run this project on Spark cluster:
(a) Run generation of the data flow (cd data_flow && python data_flow.py or cd data_flow && python data_flow_with_pyspark.py)
(b) Start Spark streaming (see (1)-(7) below)
(c) Check database: run from master "ssh db", then see SQL_server_installation.txt for more details.

(1) CHECK java 8 or INSTALL it
sudo update-alternatives --config java
java -version
go to java 1.8.*
sudo apt-get install openjdk-8-jre
In my configuration of Hadoop/Spark java 8 is used
 
(2) INSTALL sbt
echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list
echo "deb https://repo.scala-sbt.org/scalasbt/debian /" | sudo tee /etc/apt/sources.list.d/sbt_old.list
curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add
sudo apt-get update
sudo apt-get install sbt
sbt -version

(3) RESTART Spark cluster if needed
$SPARK_HOME/sbin/stop-all.sh 
$HADOOP_HOME/sbin/stop-all.sh
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh
$SPARK_HOME/sbin/start-all.sh

(4) CHECK HDFS
hdfs dfs -ls -R /scala
or
http://ip_master:9870/explorer.html#/

(5) COMPILE project
sbt package

(6) RUN Spark job
spark-submit --packages mysql:mysql-connector-java:5.1.12,org.postgresql:postgresql:42.4.3 --class "Streaming" --master yarn --deploy-mode client target/scala-2.11/streamapp2_2.11-0.1.0-SNAPSHOT.jar

(7) 
USE "--deploy-mode client" to be able to see stdout
or 
USE "--deploy-mode cluster" otherwise
