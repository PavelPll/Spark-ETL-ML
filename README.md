# OBJECTIVE: To get familiar with Spark.
I installed Spark cluster on AWS cloud and showed some its important functionalities. 
* The file **spark_installation.txt** describes the installation procedure. This is done on top of a Hadoop YARN cluster, which I installed previously (for details see Hadoop-Hive project in the current git repository: https://github.com/PavelPll/Hadoop-HIVE).  
* The folder **./Scala_ETL** contains the example of ETL (Extract Transfer Load) pipeline written in Scala, with a description of how to run it on Spark cluster.
* The folder **./Java_ML** contains the example of simple machine learning model written in Java, with a description how to run it on Spark cluster.
* The example of data batch processing with Spark (**./Streaming**).
