Objective: Run python script on Spark cluster

(1) INSTALL pip3
sudo apt install python3-pip

(2) CHECK the python on master
ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ which python3
/usr/bin/python3

(3) INSTALL virtualenv for the given ubuntu user
pip3 install virtualenv

(4) Make virtualenv running
nano .bashrc
PATH="$PATH:/home/$USER/.local/bin"
source .bashrc

(5) CREATE pysprk virtual environment and activate it
virtualenv pysprk
source pysprk/bin/activate
(pysprk) ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$

(6) INSTALL pyspark of the same version as spark
(6a) get Spark version
(pysprk) ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ spark-submit --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.4
      /_/
(6b) install pyspark
(pysprk) ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ pip3 install pyspark==3.2.4

(7) INSTALL findspark
pip install findspark

(8) INSTALL pandas
For pandas>=2.0.0 we will get later the error message AttributeError: 'DataFrame' object has no attribute 'iteritems'
so that
pip install pandas==1.5.3

(9) IMPORTANT
We will use the python installed on the master node to run the generation of continuous data. 
The python of the pysprk environment will be activated automatically for each run inside .py file (using ./pysprk/bin/activate_this.py, see data_flow_with_pyspark.py for details).
(pysprk) ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ which python3
/home/ubuntu/spark/my/st/streamApp2/data_flow/pysprk/bin/python3
(pysprk) ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ deactivate
ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$
ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ which python3
/usr/bin/python3
 
(10) Run the python script from the masternode
ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ python3 data_flow.py
or
ubuntu@masternode:~/spark/my/st/streamApp2/data_flow$ python3 data_flow_with_pyspark.py









